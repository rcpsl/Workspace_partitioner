\section{Introduction}
From simple logical constructs to complex deep neural network models, AI-agents are increasingly controlling physical/mechanical systems. Self-driving cars, drones, and smart cities are just examples of such systems to name a few. However, regardless of the explosion in the use of AI within a multitude of CPS domains, the safety and reliability of these AI-controlled CPS is still an under-studied problem. It is then unsurprising the failure of these AI-controlled CPS in several, safety-critical, situations leading to human fatalities~\cite{Accident1,Accident2,Accident3,Accident4,AccidentWiki}. 

%Recent polls show that the societal rejection of these technologies increases significantly with every failure~\cite{VergePoll} leading to more than 75\% of  Americans being too afraid to ride in a self-driving vehicle~\cite{VergePoll} and 64\% afraid from sharing the road with autonomous cars~\cite{VergePoll2}. \textbf{Such safety and reliability concerns, if not proactively addressed, will pose a significant societal barrier of adopting these technologies permanently~\cite{MITPoll,ForbesPoll}}. 


Applying formal methods to AI-controlled CPS have its unique challenges. First and foremost is the lack of widely-accepted, precise, mathematical specifications capturing the correct behavior of AI-agents, in general, and data-driven agents like deep neural networks, in particular. Instead of verifying AI-agents against \textbf{component-level specifications}, we focus instead on verifying the whole CPS against \textbf{system-level specifications} like a ``self-driving car should avoid hitting a pedestrian crossing the street''. On the one hand, system-level specifications are interpretable by end users of CPS and can directly address their safety and reliability concerns. On the other hand, verifying system-level specifications necessitates novel decision procedures capable of simultaneously reasoning about the physical and cyber components in CPS, a computationally daunting problem. 


\paragraph{Robust Learning and Adversarial examples. } New advances in AI systems have created an urgency to study safety, reliability, and potential problems that can rise and impact the society by the deployment of AI-based systems in the real world. Amodei et al. have explored concrete problems in AI safety including avoiding side effects and reward hacking, scalable supervision, safe exploration, and distributional shift~\cite{amodei2016concrete}. Malicious use of artificial intelligence~\cite{brundage2018malicious}, robustness of deep learning, and AI techniques for safety (with regards to the so-called adversarial examples) have been addressed by various groups~\cite{ferdowsi2018robust,everitt2018agi,charikar2017learning,steinhardt2017certified,munoz2017towards,paudice2018label,ruan2018global}. Unfortunately, these techniques focus mainly on the robustness of the learning algorithm with respect to data outliers without providing guarantees in terms of safety and reliability of the decisions taken by the AI-agent.


\paragraph{Formal Verification of AI-Systems. }
Due to lack of rigorous mathematical analysis of AI-agents, several researchers pointed to the urgent need of using formal methods to verify the behavior of AI-agents~\cite{kurd2003establishing,seshia2016towards,seshia2018formal,leikeAIsafety2017,leofante2018automated,scheibler2015towards}.  Several works have been reported in the last three years attempting to apply formal verification techniques to machine learning components and in particular neural networks. The work in this area can be classified into two categories namely (i) component-level and (ii) system-level verification. 
%
In the first class, component-level verification, recent works focused on verifying neural networks against input-output specifications~\cite{katz2017reluplex,ehlers2017formal,bunel2018unified,ruan2018reachability,dutta2017output,pulina2010abstraction}. Such input-output techniques compute a guaranteed range for the output of a deep neural network given a set of inputs represented as a convex polyhedron. Unfortunately, these range properties do not capture the safety and reliability of the AI-agent, especially when operated in dynamic and changing environments.


To circumvent the drawback of using simple input-output range specifications and reason directly about system safety, the second class of recent works in the literature focused on finding corner-cases that lead to the violation of system safety specifications. Unfortunately, recent works focused entirely on testing and semi-formal verification (e.g., falsification)~\cite{pei2017deepxplore,tian2017deeptest,wicker2018feature,YouchengTesting2018,LeiDeepGauge2018,Wang2018Testing,LeiDeepMutation2018,srisakaokul2018multiple,MengshiDeepRoad2018,YouchengConcolic2018,dreossi2017compositional}. While testing and falsification are useful in finding some corner-cases for which the system may fail, they lack the rigor promised by formal methods.
%In this work, we gain inspiration from these previous work to study formal verification of AI-controlled agents against system-level specifications. 
In this work, we focus instead on the problem of formal verification of AI-controlled CPS. Our objective is to provide tools capable of providing strong formal guarantees while avoiding heuristics and testing based procedures. We argue that a holistic approach is needed where the AI-agent, the continuous dynamics, and the environment can be mathematically modeled and analyzed towards verifying their behaviors.
