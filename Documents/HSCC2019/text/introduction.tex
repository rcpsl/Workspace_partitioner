\section{Introduction}
From simple logical constructs to complex deep neural network models, Artificial Intelligence (AI)-agents are increasingly controlling physical/mechanical systems. Self-driving cars, drones, and smart cities are just examples of such systems to name a few. However, regardless of the explosion in the use of AI within a multitude of cyber-physical systems (CPS) domains, the safety and reliability of these AI-enabled CPS is still an under-studied problem. It is then unsurprising the failure of these AI-controlled CPS in several, safety-critical, situations leading to human fatalities~\cite{AccidentWiki}. 

%Recent polls show that the societal rejection of these technologies increases significantly with every failure~\cite{VergePoll} leading to more than 75\% of  Americans being too afraid to ride in a self-driving vehicle~\cite{VergePoll} and 64\% afraid from sharing the road with autonomous cars~\cite{VergePoll2}. \textbf{Such safety and reliability concerns, if not proactively addressed, will pose a significant societal barrier of adopting these technologies permanently~\cite{MITPoll,ForbesPoll}}. 


Motivated by the urgency to study safety, reliability, and potential problems that can rise and impact the society by the deployment of AI-enabled systems in the real world, several works in the literature focused on the problem of designing deep neural networks models that are robust to the so-called adversarial examples~\cite{ferdowsi2018robust,everitt2018agi,charikar2017learning,steinhardt2017certified,munoz2017towards,paudice2018label,ruan2018global}. Unfortunately, these techniques focus mainly on the robustness of the learning algorithm with respect to data outliers without providing guarantees in terms of safety and reliability of the decisions taken by the neural network model. To circumvent this drawback, and motivated by the wealth of adversarial example generation approaches for neural network, recent works focused on two main techniques namely (i) testing of neural networks, (ii) falsification (semi-formal verification) of neural networks, and (iii) formal verification of neural networks.

Representatives of the first class, namely testing of neural networks, are the work reported in~\cite{pei2017deepxplore,tian2017deeptest,wicker2018feature,YouchengTesting2018,LeiDeepGauge2018,Wang2018Testing,LeiDeepMutation2018,srisakaokul2018multiple,MengshiDeepRoad2018,YouchengConcolic2018} in which the neural network is treated as a white box, and test cases are generated to maximize different coverage criteria. Such coverage criteria include neuron coverage, condition/decision coverage, and multi-granularity testing criteria. On the one hand, testing do not formally guarantee that a neural network satisfy a formal functionality, on the other hand, maximizing test coverage give system designers confidence that the networks are reasonably free from defect. %Nevertheless, the major drawback of the neural network testing is that focuses entirely on the neural network as a component without taking into consideration the system-level 


Two major drawbacks of current testing schemes of neural networks. First, they focus entirely on testing the neural network as a component without taking into consideration the effect of its decisions on the entire system behavior. Second, testing schemes do not utilize a formal notion of component-level or system-level safety specifications. This motivated researchers to focus on falsification of autonomous systems that include machine learning components~\cite{dreossi2017compositional,tuncali2018simulation,zhang2018two}. In such falsification frameworks, the objective is to generate corner test cases that will lead the whole system to violate a system-level specification. To that end, advanced 3D models and image environments are used to bridge the gap between the virtual world and the real world. By parametrizing the input to these 3D models (e.g., position of objects, position of light sources, intensity of light sources) and sampling the parameter space in a fashion that maximizes the falsification of the safety property, falsification frameworks can simulate several test cases until a counterexample is found~\cite{dreossi2017compositional,tuncali2018simulation,zhang2018two}.


While testing and falsification frameworks are powerful tools to find corner cases in which the neural network or the neural network enabled system will fail, they lack the rigor promised by formal verification methods. Therefore, several researchers pointed to the urgent need of using formal methods to verify the behavior of neural networks and neural network enabled system~\cite{kurd2003establishing,seshia2016towards,seshia2018formal,leikeAIsafety2017,leofante2018automated,scheibler2015towards}. As a result, several works have been reported in the last few years attempting to apply formal verification techniques to neural network models. 
%The work in this area can be classified into two categories namely (i) component-level verification and (ii) system-level verification. 

However, applying formal verification to neural network models comes with its unique challenges. First and foremost is the lack of widely-accepted, precise, mathematical specifications capturing the correct behavior of a neural network. Therefore, recent works focused entirely on verifying neural networks against simple input-output specifications~\cite{katz2017reluplex,ehlers2017formal,bunel2018unified,ruan2018reachability,dutta2017output,pulina2010abstraction}. Such input-output techniques compute a guaranteed range for the output of a deep neural network given a set of inputs represented as a convex polyhedron.  

To that end, several algorithm that takes advantage of the piecewise linear nature of the Rectified Linear Unit (ReLU) activation functions (one of the most famous nonlinear activation functions in deep neural networks) have been proposed recently. For example, by using binary variables to encode piecewise linear functions, the constraints of ReLU functions are encoded as a Mixed-Integer Linear Programming (MILP). Combining output specifications that are expressed in terms of Linear Programming (LP), the verification problem for output set eventually turns to the feasibility problem of MILP~\cite{dutta2018output,tjeng2017verifying}. 

Using off-the-shelf MILP solvers does not lead to a scalable approach to handle neural networks with hundreds and thousands of neurons~\cite{ehlers2017formal}. To circumvent this problem, several MILP-like solvers targeted toward the neural network verification problem are recently proposed. For example, the work reported in~\cite{katz2017reluplex} proposed a modified Simplex algorithm (originally used to solve linear programs) to take into account ReLU nonlinearities as well. Similarly, the work reported in~\cite{ehlers2017formal} combines a Boolean satisfiability solving along with a linear over-approximation of piecewise linear functions to verify ReLU neural networks against convex specifications. Other techniques that exploit specific geometric structures of the specifications are also proposed~\cite{gehr2018ai,xiang2017reachable}. A thorough survey on different algorithms for verification of neural networks against input-output range specifications can be found in~\cite{xiang2017survey} and the references within.

%For example, by taking advantage of the piecewise linear nature of ReLU activation functions, the output set computation can be formulated as operations of polytopes if the input set is given in the form of unions of polytopes. The computation process involves standard polytope operations, such as intersection and projection, and all of these can be computed by employing sophisticated computational geometry tools. However, the number of polytopes involved in the computation process increases exponentially with the number of neurons in its worst case performance which makes the method not scalable to neural networks with a large number of neurons.


%The use of binary variables to encode piecewise linear functions is standard in optimization. In [87], the constraints of ReLU functions are encoded as a Mixed-Integer Linear Programming (MILP). Combining output specifications that are expressed in terms of Linear Programming (LP), the verification problem for output set eventually turns to the feasibility problem of MILP. It is well known that MILP is an NP-hard problem and in [35, 36], the authors elucidate significant efforts for solving MILP problems efficiently to make the approach scalable. Their methods combine MILP solvers with a local search yielding a more efficient solver for range estimation problems of ReLU neural networks than several other approaches. 

%Recently, a verification engine for ReLU neural networks called AI$^2$ was proposed in [47]. In their approach, the authors abstract perturbed inputs and safety specifications as zonotopes, and reason about their behavior using operations for zonotopes. The framework AI$^2$ is capable of handling neural networks of realistic size, and, in particular, their approach has had success dealing with convolutional neural networks. In another work, a software tool, called Sherlock, was developed based on the MILP verification approaches [38]. This LP-based framework combines satisfiability (SAT) solving and linear over-approximation of piecewise linear functions in order to verify ReLU neural networks against convex specifications.


%In [76], an algorithm, that stems from the Simplex Algorithm for linear functions, for ReLU functions is proposed. Due to the piecewise linear feature of ReLU functions, each node is divided into two nodes. Thus, in their formulation, each node consists of a forward-facing and backward-facing node. If the ReLU semantics are not satisfied, two additional update functions are given to fix the mismatching pairs. Thus, the search process is similar to the Simplex Algorithm that pivots and updates the basic and non-basic variables with the addition of a fixing process for ReLU activation pairs.






% First and foremost is the lack of widely-accepted, precise, mathematical specifications capturing the correct behavior of the neural network. Instead of verifying AI-agents against \textbf{component-level specifications}, we focus instead on verifying the whole CPS against \textbf{system-level specifications} like a ``self-driving car should avoid hitting a pedestrian crossing the street''. On the one hand, system-level specifications are interpretable by end users of CPS and can directly address their safety and reliability concerns. On the other hand, verifying system-level specifications necessitates novel decision procedures capable of simultaneously reasoning about the physical and cyber components in CPS, a computationally daunting problem. 


%\paragraph{Robust Learning and Adversarial examples. } New advances in AI systems have created an urgency to study safety, reliability, and potential problems that can rise and impact the society by the deployment of AI-based systems in the real world. Amodei et al. have explored concrete problems in AI safety including avoiding side effects and reward hacking, scalable supervision, safe exploration, and distributional shift~\cite{amodei2016concrete}. Malicious use of artificial intelligence~\cite{brundage2018malicious}, robustness of deep learning, and AI techniques for safety (with regards to the so-called adversarial examples) have been addressed by various groups~\cite{ferdowsi2018robust,everitt2018agi,charikar2017learning,steinhardt2017certified,munoz2017towards,paudice2018label,ruan2018global}. Unfortunately, these techniques focus mainly on the robustness of the learning algorithm with respect to data outliers without providing guarantees in terms of safety and reliability of the decisions taken by the AI-agent.


%\paragraph{Formal Verification of AI-Systems. }
%Due to lack of rigorous mathematical analysis of AI-agents, several researchers pointed to the urgent need of using formal methods to verify the behavior of AI-agents~\cite{kurd2003establishing,seshia2016towards,seshia2018formal,leikeAIsafety2017,leofante2018automated,scheibler2015towards}.  

%Several works have been reported in the last three years attempting to apply formal verification techniques to machine learning components and in particular neural networks. The work in this area can be classified into two categories namely (i) component-level and (ii) system-level verification. 
%
%In the first class, component-level verification, recent works focused on verifying neural networks against input-output specifications~\cite{katz2017reluplex,ehlers2017formal,bunel2018unified,ruan2018reachability,dutta2018output,pulina2010abstraction}. Such input-output techniques compute a guaranteed range for the output of a deep neural network given a set of inputs represented as a convex polyhedron. Unfortunately, these range properties do not capture the safety and reliability of the AI-agent, especially when operated in dynamic and changing environments.


%To circumvent the drawback of using simple input-output range specifications and reason directly about system safety, the second class of recent works in the literature focused on finding corner-cases that lead to the violation of system safety specifications. Unfortunately, recent works focused entirely on testing and semi-formal verification (e.g., falsification)~\cite{pei2017deepxplore,tian2017deeptest,wicker2018feature,YouchengTesting2018,LeiDeepGauge2018,Wang2018Testing,LeiDeepMutation2018,srisakaokul2018multiple,MengshiDeepRoad2018,YouchengConcolic2018,dreossi2017compositional}. While testing and falsification are useful in finding some corner-cases for which the system may fail, they lack the rigor promised by formal methods.
%In this work, we gain inspiration from these previous work to study formal verification of AI-controlled agents against system-level specifications. 





Unfortunately, the input-output range properties, studied so far in the literature, are simplistic and fails to capture the safety and reliability of the whole system. Therefore, in this paper, we focus instead on the problem of formal verification of a neural network controlled robot against system-level safety specifications. In particular, we consider the problem in which a robot utilizes a LiDAR scanner to sense its environment. The LiDAR image is then processed by a neural network controller which computes the control actions based on the current LiDAR image. Such scenario is common in the literature of behavioral cloning and imitation control in which the neural network is trained to imitate the actions of experts who manually controlled the robot~\r{\cite{??}}. With the objective to verify the safety of this robot, we develop a framework that can take into account the robot continuous dynamics, the workspace configuration, the LiDAR imaging, and the neural network, and compute the set of initial states for the robot that is guaranteed to produce robot trajectories that satisfy the safety specification.


To carry out the prescribed formal verification problem, we need a mathematical model that captures the LiDAR imaging process. This is the process that generates the LiDAR images based on the robot pose along with the workspace objects.  Therefore, the first contribution of this paper is to show that, under mild assumptions on the workspace, the robot dynamics, and the pre-processing of the LiDAR images, the mathematical model of the LiDAR imaging process enjoys favorable mathematical structure that renders it amenable to formal verification. In particular, we show that the LiDAR imaging process can be modeled as a piecewise affine function and we develop a polynomial-time algorithm that can partition the workspace into regions according to the imaging function.

%Unfortunately, the number of partitioned regions depend on the number of laser rays used by the LiDAR scanner. 

Given the partitioned workspace along with a pre-trained neural network and the robot dynamics, we compute a finite state abstraction of the closed loop dynamics. Such abstraction will be used later to verify the safety specifications. Similar to previous works in the literature, we strict our focus to neural networks with Rectified Linear Unit (ReLU) nonlinearities and we develop a Satisfiability Modulo Convex (SMC) programming algorithm that uses a combination of a Boolean satisfiability solver and a convex programming solver to iteratively reason about the neural network nonlinearity along with the dynamics and the imaging constraints. At each iteration, the boolean satisfiability solver searches for a candidate assignment for the ReLU phases while ignoring the neural network weights, the robot dynamics, and the LiDAR imaging. The convex programming solver is then used to check the feasibility of the proposed ReLU phase assignment against the neural network weights, the robot dynamics, and the LiDAR imaging. If the ReLU phase assignment is deemed infeasible, then the SMC solver will generate succinct explanations for their infeasibility to reduce the search space. 

Unfortunately, the SMC algorithm needs to be executed while taking into considerations each combination of regions in the partitioned workspace. Since the number of regions grows exponentially with the number of laser rays used by the LiDAR scanner, our framework utilizes an abstraction refinement process in which coarse abstractions based on a smaller number of laser rays are first considered and the refined if deemed necessary by the SMC-based procedure. Our simulation results show that our framework is capable of proving the safety of an autonomous robot with \r{XX} continuous states controlled by a neural network that consists of \r{XX} neurons. To summarize, the contributions of this paper can be summarized as follows:\\
\textbf{1-} A framework for formally proving safety properties of autonomous robots controlled by neural network processing LiDAR images.\\
\textbf{2-} An algorithm for computing a piece-wise affine representation of the LiDAR imaging process. \\
\textbf{3-} An SMC-based algorithm combined with an abstraction refinement process for computing finite abstractions of the neural network controlled autonomous robot.

%The remainder of this paper is organized as follows. In Section~\ref{??} we give .. In Section~\ref{??}..









