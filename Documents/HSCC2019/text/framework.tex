\section{Framework}

The system-level neural network verification problem appears to have two challenges:
ReLU activation function is nonlinear, and LiDAR image dependence on position of robot is also nonlinear.
We tackle the first one by developing a SMC Programming algorithm,
and the second one by partitioning workspace into regions followed by building a state machine 
that captures transition feasibility between regions.


\subsection{SMC Programming}

SMC Programming is designed to efficiently reason about Boolean and convex constraints 
at the same time~\cite{Shoukry2018SMC} by integrating SAT solving and convex optimization.
This approach is a natural choice in reasoning NN due to the piecewise linearity of ReLU activation function.
Specifically, a Boolean satisfiability solver assigns the phases of neurons, and a convex programming solver 
determines if all real-valued constraints can be satisfied. 
For a certain selection of ReLU phases, NN becomes an affine function with limit on NN inputs to satisfy the selected phases. 
Therefore, the nonlinear function represents NN \eqref{eq:nn_nonlinear} is equivalent to the following affine constraints:
\begin{equation}
    \label{eq:nn_linear}  
    u_t =  G_b d_t + h_b, \quad Q_b d_t \le c_b, \qquad \forall t \in \N 
\end{equation}
where $G_b$, $h_b$, $Q_b$, $c_b$ depend on the selection of ReLU phases, and have appropriate dimensions 
determined by NN and its input size.




\subsection{LiDAR Image Function}

In general, LiDAR image dependence on robot position is nonlinear even with workspace fixed.
This is because obstacle boundary intersects laser in each direction could be different when robot is at positions.
However, one can imagine when robot is constrained in a small region, the intersecting obstacles may be fixed.
In this subsection, we formally define the collection of obstacles detected by a LiDAR, 
claim that LiDAR image is a piecewise affine function, 
and propose an algorithm to find subdomains in which LiDAR image function is affine.


\begin{definition}
    A LiDAR configuration is a list of obstacle boundaries detected by a LiDAR in a collection of sorted directions.
\end{definition}

\begin{theorem}
    For a given workspace with polyhedral obstacles, if detecting directions of LiDAR are fixed, 
    and LiDAR detection range is large enough such that laser in every direction intersects an obstacle boundary, 
    then LiDAR image is a piecewise affine function of robot position:
    \begin{equation}
        \label{eq:image_func}
        d_t = P_j [x_{p_t} \quad y_{p_t}]^\intercal + q_j, \quad \forall j \in {1, ..., M} 
    \end{equation}
    where $M$ is number of regions identified by LiDAR configuration.
\end{theorem}    

To prove this theorem, we propose an algorithm that partition workspace based on LiDAR configuration.

\begin{proof}
    First, partition free space by line segments that have one endpoint be obstacle vertices, one endpoint on obstacle boundaries, 
    and oriented along LiDAR detecting directions.
    Such partition line segments can be found by assuming rays start from obstacle vertices,
    in all detecting directions except those not point to free space.
    Intersection between a ray and the first obstacle boundary it encounters is the other endpoint of a partition line segment.
    Next, all intersections between partition line segments can be found by a sweep algorithm~\cite{CGbook}. 
    {\color{blue} More detail about sweep if necessary.}
    Consider a graph consists of all partition line segments and intersection points,
    then each region that cannot be further divided can be found by graph search.
    Each of these regions corresponds to a unique LiDAR configuration.
    
    Assume robot equipped with a LiDAR scanner is constrained in an arbitrary one of these regions, 
    then obstacle boundary detected in each direction does not depend on the position of robot 
    due to the uniqueness of LiDAR configuration for each region.
    In other words, for each detecting direction $\theta^i \in {1, ..., N}$, there exists $0 \le k^i \le 1$ such that:
    \begin{equation}
        \label{eq: intersection1}
        x^i = x_{p_t} + k^i R cos\theta^i, \qquad y^i = y_{p_t} + k^i R sin\theta^i, \\
    \end{equation} 
    \begin{equation}
        \label{eq: intersection2}
        x_{obs}^s \le x^i \le x_{obs}^l, \qquad y_{obs}^s \le y^i \le y_{obs}^l
    \end{equation} 
    where constant $R$ is the maximum detection range of LiDAR.
    Constants $x_{obs}^s$, $x_{obs}^l$, $y_{obs}^s$, $y_{obs}^l$ parameterize the obstacle boundary detected in direction $\theta^i$.
    Since detecting directions are fixed when robot moves around,  
    $\theta^i$ are constants and hence $x^i$, $y^i$ are affine functions of robot position $(x_{p_t}$, $y_{p_t})$.
    Therefore, LiDAR image $d_t$ that consists of linear terms of $x^i$, $y^i$ is an affine function of robot position.
\end{proof}

As a special case, consider obstacles are all rectangles. 
A laser in direction $\theta^i$, where $i \in {1, ..., N}$, intersects a vertical obstacle 
defined by lower endpoint $(x_{obs}, y_{obs}^s)$ and upper endpoint $(x_{obs}, y_{obs}^l)$, 
constraints ~\eqref{eq: intersection1}, ~\eqref{eq: intersection2} can be simplified as:
\begin{equation}
    \label{eq:vertical}
    x^i = x_{obs}, \qquad y^i = y_{p_t} + (x_{obs} - x_{p_t}) tan \theta^i
\end{equation}
Similarly, for a horizontal obstacle boundary with left endpoint $(x_{obs}^s, y_{obs})$ and right endpoint $(x_{obs}^l, y_{obs})$,
constraints ~\eqref{eq: intersection1}, ~\eqref{eq: intersection2} can be written as:
\begin{equation} 
    \label{eq:horizontal}
    x^i = x_{p_t} + (y_{obs} - y_{p_t}) cot \theta^i, \qquad y^i = y_{obs}
\end{equation}


\begin{theorem}
    Given workspace with polyhedral obstacles, running time of partitioning workspace based on LiDAR configuration is
    $\mathcal{O}(n\log{}n + I\log{}n)$, where $n$ is number of partition line segments 
    and $I$ is number of intersection points of segments.
    \begin{proof}
        {\color{blue} This is complexity of sweep algorithm.} 
    \end{proof}
\end{theorem}    

{\color{blue} In practice have numerical errors.}




\subsection{Finite Transition abstraction}

To solve the system-level neural network verification problem, 
we build a finite transition abstraction that can be used to verify system property $\varphi$.
Each state of the finite transition abstraction represents a regions with unique LiDAR configuration,
and transition feasibility between a given pair of regions can be determined by solving a feasibility problem 
with constraints ~\eqref{eq:dyn}, ~\eqref{eq:ulimit}, ~\eqref{eq:nn_nonlinear}, ~\eqref{eq:image_func}.
Since constraints ~\eqref{eq:nn_nonlinear} is nonlinear, we adopt SMC Programming to
divide the feasibility problem into subproblems with constraints ~\eqref{eq:nn_linear}.



{\color{blue} How to use state machine to verify LTL?}
{\color{blue} Any limit on LTL formula?}

{\color{blue} Talk about abstraction refinement.}








\begin{comment}
Thus, we add constraints that quadrotor is located in region $R_j$ currently and moves to region $R_{j^\prime}$,
$\forall j \neq j^\prime \in {1, ..., M}$:
\begin{equation}
    \label{eq:region}
    E_j[x_{p_t} \quad y_{p_t}]^\intercal <= f_j, \qquad E_{j^\prime}[x_{p_{t+1}} \quad y_{p_{t+1}}]^\intercal <= f_{j^\prime}
\end{equation}
\end{comment}




