\section{Problem Formulation}

\subsection{Notation}

We denote with $x = (x^1, x^2, ..., x^n)$ a column vector of real-valued variables, where $x^i \in \R$,
and with $t \in \N$ time steps are non-negative integers. 
When not directly inferred from the context, $max$ denotes element-wise maximum between two vectors, 
and $\norm{\cdot}$ denotes the infinity norm of a vector.


\subsection{Dynamics and Workspace}

We assume that the dynamics of a robot is described by a discrete-time linear system of the form:

\begin{equation}
    \label{eq:dyn}    
    x_{t+1} = A x_{t} + B u_{t}, \qquad y_{t} = C x_{t} 
\end{equation}

%\begin{equation}
%    \label{eq:ulimit}    
%    \norm{x_t} \le \overline{x}, \qquad \norm{u_t} \le \overline{u}, \quad \forall t \in \N
%\end{equation}
where $x_t \in \mathcal{X} \subseteq \R^{n}$ is the state of robot at time $t \in \N$, 
$y_t \in \R^{2}$ is the position of robot consists of $x$ and $y$ coordinates,
$u_t \in \mathcal{U} \subseteq \R^{m}$ is the robot input.
%and $\overline{u}$ and $\overline{x}$ are bounds on the input and state variables. 
The matrices $A$ and $B$ represent the robot dynamics and have appropriate dimensions. 
For a robot with nonlinear dynamics that is either differentially flat or feedback linearizable, 
the state space model~\eqref{eq: dyn} corresponds to its feedback linearized dynamics.

We consider robot in a workspace $\W \subset \R^{w}$ where $w$ can be $2$ or $3$, 
corresponding,  respectively, to a $2$-dimensional or $3$-dimensional workspace. 
Assume that robot must avoid a set of \emph{obstacles} $\mathcal{O} = \{\mathcal{O}_1, \ldots, \mathcal{O}_o\}$, 
with $\mathcal{O}_i \subset \R^w$ is assumed to be polyhedron.
We denote obstacle boundaries for both boundaries of obstacles and workspace, and similarly for obstacle vertices. 



\subsection{LiDAR Image}

Consider an autonomous robot is equipped with a 1-d LiDAR scanner $\mathcal{L}$ 
that emits a set of $N$ lasers arranged in a $360$ degree fan.
The observation signal $r_t$ at time $t$ is distances measured in a set of LiDAR detecting directions $\theta \in \R^N$.
The dependence of observation $r_t$ on the position of robot and workspace is captured by the LiDAR image function:
\begin{equation}
    \label{eq:image_func_nonlinear}
    r_t = g_{\mathcal{L}}(y_t, \W), \qquad \forall t \in \N
\end{equation}    
Furthermore, we consider LiDAR image $d_t \in \R^{2N}$ generated by preprocessing observation $r_t$ in the following way:
\begin{equation}
    \label{eq:image}
    d_t^i =
    \begin{cases} 
      r_t^i cos \theta^i  & \quad \forall i \in \{1, ..., N\} \\
      r_t^{i-N} sin \theta^{i-N}  & \quad \forall i \in \{N+1, ..., 2N\} 
    \end{cases}
    \qquad \forall t \in \N
\end{equation}

We emphasize that the LiDAR image function \eqref{eq:image_func_nonlinear} is nonlinear 
because obstacle boundary intersects each laser could be different when the robot is at different positions.
However, $d_t$ dependence on $r_t$ is linear since the set of detecting directions $\theta$ is pre-defined and fixed.




\subsection{Neural Network}

A neural network is comprised of multiple layers including an output layer at the end.
In order to represent nonlinear relations, an activation function is applied to each layer except the output layer of NN.
In this paper, we consider fully connected layers with Rectified Linear Unit (ReLU)~\cite{Hinton2010ReLU} activation function.
Every neuron in a fully connected layer computes a weighted summation of all neurons in the preceding layer, 
and ReLU returns the maximum of the weighted summation and zero, which can be written as $ReLU(x) = max(x, 0)$.

Consider a NN architecture with $L$ fully connected layers and $N_l$, $\forall l \in \{1,...L\}$, neurons in each layer.
The $l^{th}$ layer can be described by a weight matrix $W_l \in \R^{N_l \times N_{l-1}}$ and a bias vector $b \in \R^{N_l}$,
which are determined during training phase.
Thus, function computed by the $l^{th}$ layer can be written as $f_l(z) = ReLU(W_l z + b_l)$, $\forall l \in {1, ..., N-1}$, 
and $f_L(z) = W_L z + b_L$, where $z$ is input to NN or the output of the preceding layer.
The function computed by a given NN $\mathcal{N}$ can be therefore written as the composition: 
$f_{\mathcal{N}} = f_L \circ ... \circ f_1$.

We consider NN as a feedback controller that makes decision based on current states of robot and observation of environment:
\begin{equation}    
    \label{eq:nn_nonlinear}    
    u_t = f_{\mathcal{N}}(d_t), \quad \forall t \in \N
\end{equation}    





\subsection{Problem Definition}
We first define NN-controlled system, which is comprised of robot dynamics, a LiDAR scanner, 
and a NN-controller that processes LiDAR image.

\begin{definition}
    \textit{(NN-controlled system)}: 
    A NN-controlled system is defined by a tuple 
    $\mathcal{S} = \langle (A, B), \mathcal{L}, \mathcal{N} \rangle$, where:
    \begin{itemize} 
        \item ${A, B}$ is the robot dynamics,
        %\item $\overline{u}$ is the bound on the robot inputs,
        %\item $\overline{x}$ is the bound on the robot states,
        \item $\mathcal{L}$ is a LiDAR scanner that measures distances to obstacles in a set of fixed directions,
        \item $\mathcal{N}$ is a pre-trained NN consists of fully connected layers and applies ReLU activation function.
    \end{itemize}
\end{definition}    


We now formally define system-level neural network verification problem that we solve in this paper:
\begin{definition}
    \textit{(System-Level Neural Network Verification Problem)}: 
    Given a NN-controlled system $\mathcal{S} = \langle (A, B), \mathcal{L}, \mathcal{N} \rangle$, 
    a workspace $\W$, and a LTL formula $\varphi$, the system-level neural network verification problem is to find 
    a region $\mathcal{W}_0 \subseteq \W$ such that 
    constraints ~\eqref{eq:dyn}, ~\eqref{eq:image_func_nonlinear}, ~\eqref{eq:image}, ~\eqref{eq:nn_nonlinear}
    and $\varphi$ are guaranteed to be satisfied by the NN-controlled system $\mathcal{S}$
    as long as the initial position of robot is in $\mathcal{W}_0$.
\end{definition}    


\subsection{Linear Temporal Logic}

{\color{blue} LTL background.}



